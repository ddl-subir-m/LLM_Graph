{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7643313-b095-4401-8e97-bbd3a476bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from statistics import median\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9b9c6-0703-4320-8422-96e3f48aa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "EVAL_PROMPT = \"\"\"\n",
    "Your goal is give an integer score in the range of 1 to 10 by matching how well the LLM Answer. While assigning a score just focus on semantic matching with the Ground Truth Answer. A bad answer will have a low integer score and a good match will have a high integer score\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "LLM Answer:\n",
    "{llm_answer}\n",
    "\n",
    "Ground Truth Answer:\n",
    "{gt_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=EVAL_PROMPT\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65ad9abf-7fe2-42a4-9d01-8c0a44b1a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_match_scores_distributions(graph_rag_scores, rag_scores):\n",
    "    # Set the figure size and adjust the padding between and around the subplots\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the first set of match scores\n",
    "    plt.hist(graph_rag_scores, bins=10, alpha=0.5, color='#aec6cf', label='Graph RAG')\n",
    "    \n",
    "    # Plot the second set of match scores\n",
    "    plt.hist(rag_scores, bins=10, alpha=0.5, color='#ff6961', label='RAG')\n",
    "    \n",
    "    # Adding titles and labels\n",
    "    plt.title('Distribution of Match Scores')\n",
    "    plt.xlabel('Match Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Display legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute and print the median for both sets of match scores\n",
    "    median_graph_rag_score = median(graph_rag_scores)\n",
    "    median_rag_score = median(rag_scores)\n",
    "    print(f\"The median match score for Graph RAG is: {median_graph_rag_score}\")\n",
    "    print(f\"The median match score for RAG is: {median_rag_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f67a83c-d515-492a-bcaa-2c5eb3f42458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_scores(csv_file_name):\n",
    "    # Step 1: Read the CSV file\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    \n",
    "    # Step 2: Extract column values\n",
    "    llm_answers = df['llm_answer'].tolist()\n",
    "    gt_answers = df['gt_answer'].tolist()\n",
    "    \n",
    "    # Step 3: Format the data\n",
    "    batch_data = [{\"llm_answer\": llm_answer, \"gt_answer\": gt_answer, \"format_instructions\":format_instructions} for llm_answer, gt_answer in zip(llm_answers, gt_answers)]\n",
    "    \n",
    "    # Step 4: Call chain.batch\n",
    "    results = chain.batch(batch_data)   \n",
    "    \n",
    "    match_scores = extract_match_scores(results)\n",
    "    return match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87b26b4d-b820-4a97-b272-c8b16b1c783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_match_scores(result):\n",
    "    # match_scores = [eval(result['text'])['match_score'] for result in results]\n",
    "    match_scores = []\n",
    "    for result in results:\n",
    "        # Parse the JSON string in the 'text' field\n",
    "        parsed_text = json.loads(result['text'])\n",
    "        # Extract the 'match_score' and add it to the list\n",
    "        match_scores.append(parsed_text['match_score'])\n",
    "    \n",
    "    return(match_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b51247a-bedf-4f16-9bff-84957bcfc4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"match_score\": {\"title\": \"Match Score\", \"description\": \"A scalar value representing how well the answer matches the ground truth\", \"type\": \"integer\"}, \"eval_description\": {\"title\": \"Eval Description\", \"description\": \"An explanation that justifies the score that was assigned for how well the answer matches ground truth\", \"type\": \"string\"}}, \"required\": [\"match_score\", \"eval_description\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class EvalResultSchema(BaseModel):\n",
    "\n",
    "    match_score:int = Field(description=\"A scalar value representing how well the answer matches the ground truth\")\n",
    "    eval_description:str = Field(description=\"An explanation that justifies the score that was assigned for how well the answer matches ground truth\")\n",
    "\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=EvalResultSchema)\n",
    "format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "# The Pydantic model creates the formatting instructions to be included in the prompt\n",
    "# Here is the what those instructions look like\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da902f-853f-4072-b8e3-1a29d44ed099",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_scores = get_match_scores(\"graph_rag_eval.csv\")\n",
    "rag_scores = get_match_scores(\"rag_eval.csv\")\n",
    "plot_match_scores_distributions(graph_rag_scores, rag_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
